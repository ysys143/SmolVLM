# SmolVLM-500M-Instruct + llama.cpp (Multimodal Inference)

This repository demonstrates how to run the [SmolVLM-500M-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-500M-Instruct) vision-language model using [llama.cpp](https://github.com/ggerganov/llama.cpp) with image input.

---

## âœ… Prerequisites

- macOS (Apple Silicon) or Linux
- CMake â‰¥ 3.22
- `git`, `wget`

---

## ğŸ”§ 1. Install llama.cpp with Metal (Mac) or CPU/GPU (Linux)

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build && cd build
cmake -DLLAMA_METAL=on ..         # Use -DLLAMA_CUBLAS=on for CUDA
cmake --build . --config Release
```

---

## ğŸ“¦ 2. Download SmolVLM-500M-Instruct (Q8\_0) and mmproj

```bash
mkdir -p ~/llama.cpp/models
cd ~/llama.cpp/models

# Model
wget -O SmolVLM-500M-Instruct-Q8_0.gguf \
https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-Q8_0.gguf

# Multimodal projection
wget -O mmproj-SmolVLM-500M-Instruct-Q8_0.gguf \
https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf
```

---

## ğŸš€ 3. Launch the llama server

```bash
cd ~/llama.cpp/build/bin

./llama-server \
  -m ../../models/SmolVLM-500M-Instruct-Q8_0.gguf \
  --mmproj ../../models/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf \
  -ngl 1     # Use -ngl 0 for CPU-only
```

Server will start on `http://localhost:8080`.

### ìŠ¤í¬ë¦½íŠ¸ë¡œ ì„œë²„ ì‹œì‘í•˜ê¸°

ë” í¸ë¦¬í•˜ê²Œ ì„œë²„ë¥¼ ì‹œì‘í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ìŠ¤í¬ë¦½íŠ¸ ë§Œë“¤ê¸°
mkdir -p ~/bin
cat > ~/bin/start-smolVLM << 'EOL'
#!/bin/bash

# SmolVLM-500M ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
cd ~/llama.cpp/build/bin

./llama-server \
  -m ../../models/SmolVLM-500M-Instruct-Q8_0.gguf \
  --mmproj ../../models/mmproj-SmolVLM-500M-Instruct-Q8_0.gguf \
  -ngl 1     # GPU ì‚¬ìš©. CPUë§Œ ì‚¬ìš©í•˜ë ¤ë©´ -ngl 0ìœ¼ë¡œ ë³€ê²½

echo "SmolVLM-500M ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
EOL

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x ~/bin/start-smolVLM

# PATHì— ì¶”ê°€ (ë§Œì•½ ì•„ì§ ì¶”ê°€ë˜ì§€ ì•Šì•˜ë‹¤ë©´)
echo 'export PATH="$HOME/bin:$PATH"' >> ~/.zshrc
source ~/.zshrc
```

ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ê°„ë‹¨íˆ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
start-smolVLM
```

---

## ğŸŒ 4. Open the Web UI

```bash
open ../examples/server/public/index.html
```

Or open manually in browser:
`file:///Users/yourname/llama.cpp/examples/server/public/index.html`

---

## ğŸ“¤ 5. API Request Format (example)

```json
POST /v1/chat/completions
Content-Type: application/json

{
  "messages": [
    {
      "role": "user",
      "content": [
        { "type": "text", "text": "What do you see?" },
        { "type": "image_url", "image_url": { "url": "data:image/jpeg;base64,..." } }
      ]
    }
  ],
  "max_tokens": 100
}
```

---

## âœ… Example Result

```
Asian man in front of a mirror looking at himself.
```

---

## Notes

* Requires llama.cpp after [PR #13050](https://github.com/ggerganov/llama.cpp/pull/13050)
* For real-time video/image input, use camera + canvas â†’ base64 â†’ API

---

```
